#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_timestamp, lit
from pyspark.sql.types import StructType, StructField, DoubleType, StringType
import os
import sys
import argparse
import logging
import psycopg2
from psycopg2 import pool
from psycopg2.extras import execute_batch
from datetime import datetime, timedelta
from dotenv import load_dotenv
load_dotenv("/opt/spark-data/.env")

# -----------------------------------------------------
# Global Connection Pool (Executorë‹¹ 1ê°œì”© ìƒì„±ë¨)
# -----------------------------------------------------
_connection_pool = None

def get_connection_pool(args):
    """
    Connection Pool ì‹±ê¸€í†¤ íŒ¨í„´
    ê° Executor JVMì—ì„œ ìµœì´ˆ 1íšŒë§Œ ìƒì„±
    """
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=5,  # Executorë‹¹ ìµœëŒ€ 5ê°œ ì—°ê²°
            host=args.pg_host,
            port=args.pg_port,
            dbname=args.pg_db,
            user=args.pg_user,
            password=args.pg_pass,
            # ì—°ê²° ìµœì í™” ì˜µì…˜
            connect_timeout=10,
            options='-c statement_timeout=30000'  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        )
    return _connection_pool


# -----------------------------------------------------
# Logger
# -----------------------------------------------------
def setup_logger():
    log_dir = "/tmp/spark-logs"
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logger = logging.getLogger("spark_batch_job")
    logger.setLevel(logging.INFO)

    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(fmt)
    ch.flush = sys.stdout.flush

    fh = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    fh.setFormatter(fmt)

    if logger.hasHandlers():
        logger.handlers.clear()
    logger.addHandler(ch)
    logger.addHandler(fh)

    logger.info(f"Logging started: {log_path}")
    return logger


# -----------------------------------------------------
# í…Œì´ë¸” ìƒì„± í•¨ìˆ˜ (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)
# -----------------------------------------------------
def ensure_table_exists(args):
    """Driverì—ì„œ í…Œì´ë¸” ìƒì„± (1íšŒë§Œ)"""
    conn = psycopg2.connect(
        host=args.pg_host,
        port=args.pg_port,
        dbname=args.pg_db,
        user=args.pg_user,
        password=args.pg_pass
    )
    cur = conn.cursor()

    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {args.pg_table} (
        send_timestamp TIMESTAMPTZ,
        machine TEXT,
        timestamp TEXT,
        usage TEXT,
        label INT,
        {','.join([f'col_{i} FLOAT' for i in range(38)])},
        PRIMARY KEY (machine, timestamp, usage)
    );
    """
    
    # ì¸ë±ìŠ¤ ìƒì„± (ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ)
    index_sql = f"""
    CREATE INDEX IF NOT EXISTS idx_{args.pg_table}_send_timestamp 
    ON {args.pg_table}(send_timestamp DESC);
    """
    
    cur.execute(create_sql)
    cur.execute(index_sql)
    conn.commit()
    cur.close()
    conn.close()


# -----------------------------------------------------
# Partitionë³„ PostgreSQL ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬)
# -----------------------------------------------------
def write_partition_to_postgres(partition_iter, args):
    """
    ê° Executorì—ì„œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜
    Connection Poolì—ì„œ ì—°ê²°ì„ ê°€ì ¸ì™€ ë³‘ë ¬ë¡œ ì €ì¥
    """
    import logging
    
    # Executor ë¡œê¹… ì„¤ì •
    logger = logging.getLogger("executor")
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s [EXECUTOR] %(message)s"))
        logger.addHandler(handler)
    
    conn = None
    try:
        # Connection Poolì—ì„œ ì—°ê²° ê°€ì ¸ì˜¤ê¸°
        pool = get_connection_pool(args)
        conn = pool.getconn()
        cur = conn.cursor()
        
        # ì»¬ëŸ¼ ì´ë¦„
        cols = [
            "send_timestamp", "machine", "timestamp", "usage", "label",
            *[f"col_{i}" for i in range(38)]
        ]
        placeholders = ",".join(["%s"] * len(cols))
        
        upsert_sql = f"""
        INSERT INTO {args.pg_table} ({','.join(cols)})
        VALUES ({placeholders})
        ON CONFLICT (machine, timestamp, usage)
        DO NOTHING;
        """
        
        batch_records = []
        BATCH_SIZE = 500
        total_rows = 0
        
        for row in partition_iter:
            record = [
                row.send_timestamp,
                row.machine,
                row.timestamp,
                row.usage,
                row.label
            ]
            for i in range(38):
                record.append(getattr(row, f"col_{i}"))
            
            batch_records.append(tuple(record))
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì»¤ë°‹
            if len(batch_records) >= BATCH_SIZE:
                execute_batch(cur, upsert_sql, batch_records, page_size=BATCH_SIZE)
                conn.commit()
                total_rows += len(batch_records)
                batch_records.clear()
        
        # ë‚¨ì€ ë ˆì½”ë“œ ì²˜ë¦¬
        if batch_records:
            execute_batch(cur, upsert_sql, batch_records, page_size=len(batch_records))
            conn.commit()
            total_rows += len(batch_records)
        
        cur.close()
        logger.info(f"Partition ì €ì¥ ì™„ë£Œ: {total_rows} rows")
        
    except Exception as e:
        logger.error(f"Partition ì €ì¥ ì‹¤íŒ¨: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        # Connection Poolì— ì—°ê²° ë°˜í™˜
        if conn:
            pool = get_connection_pool(args)
            pool.putconn(conn)


# -----------------------------------------------------
# foreachBatch í•¸ë“¤ëŸ¬
# -----------------------------------------------------
def process_batch(batch_df, batch_id, args, logger):
    """
    ê° ë§ˆì´í¬ë¡œë°°ì¹˜ë§ˆë‹¤ ì‹¤í–‰
    Kafka partitionì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì  ì„±ëŠ¥ ë‹¬ì„±
    """
    row_count = batch_df.count()
    if row_count == 0:
        logger.info(f"[Batch {batch_id}] ë°ì´í„° ì—†ìŒ â†’ Skip")
        return
    
    num_partitions = batch_df.rdd.getNumPartitions()
    logger.info(f"[Batch {batch_id}] ì‹œì‘ - Partitions: {num_partitions}, Rows: {row_count}")
    
    # Kafka partition ê·¸ëŒ€ë¡œ ì‚¬ìš© (shuffle ì—†ìŒ, ìµœê³  ì„±ëŠ¥)
    batch_df.foreachPartition(lambda partition: write_partition_to_postgres(partition, args))
    
    logger.info(f"[Batch {batch_id}] ì²˜ë¦¬ ì™„ë£Œ")


# -----------------------------------------------------
# ì¸ì íŒŒì„œ
# -----------------------------------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Spark Streaming with PostgreSQL Connection Pool")

    parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"))
    parser.add_argument("--pg-port", default=os.getenv("PG_PORT", "5432"))
    parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"))
    parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"))
    parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"))
    parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_data_lake"))

    parser.add_argument("--kafka-bootstrap", default=os.getenv("KAFKA_BOOTSTRAP", "kafka.kafka.svc.cluster.local:9092"))
    parser.add_argument("--topic", default=os.getenv("KAFKA_TOPIC", "server-machine-usage"))

    parser.add_argument("--checkpoint-location", default="/tmp/spark-checkpoint")
    parser.add_argument("--trigger-interval", default="10 seconds", help="ë§ˆì´í¬ë¡œë°°ì¹˜ ê°„ê²©")

    return parser.parse_args()


# -----------------------------------------------------
# Main
# -----------------------------------------------------
def main():
    logger = setup_logger()
    args = parse_args()

    logger.info("=" * 60)
    logger.info("Spark Streaming with Connection Pool ì‹œì‘")
    logger.info(f"Kafka: {args.kafka_bootstrap}")
    logger.info(f"Topic: {args.topic}")
    logger.info(f"PostgreSQL: {args.pg_host}:{args.pg_port}/{args.pg_db}")
    logger.info(f"Table: {args.pg_table}")
    logger.info("=" * 60)

    # í…Œì´ë¸” ìƒì„± (Driverì—ì„œ 1íšŒë§Œ)
    ensure_table_exists(args)
    logger.info("í…Œì´ë¸” ìƒì„±/í™•ì¸ ì™„ë£Œ")

    # Kafka JSON ìŠ¤í‚¤ë§ˆ
    schema = StructType([
        StructField("send_timestamp", StringType(), True),
        StructField("machine", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("usage", StringType(), True),
        StructField("label", DoubleType(), True),
        *[StructField(f"col_{i}", DoubleType(), True) for i in range(38)],
    ])

    spark = (
        SparkSession.builder
        .appName("SparkStreamingConnectionPool")
        .config("spark.sql.session.timeZone", "Asia/Seoul")
        .config("spark.streaming.kafka.consumer.cache.enabled", "false")
        .getOrCreate()
    )
    # ğŸ”‡ Spark ë‚´ë¶€ INFO ë¡œê·¸ ì œê±°
    spark.sparkContext.setLogLevel("ERROR")

    # Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", args.kafka_bootstrap) \
        .option("subscribe", args.topic) \
        .option("startingOffsets", "latest") \
        .option("maxOffsetsPerTrigger", "10000") \
        .option("failOnDataLoss", "false") \
        .load()

    # JSON íŒŒì‹± ë° íƒ€ì… ë³€í™˜
    json_df = df.selectExpr("CAST(value AS STRING) as json_str") \
        .select(from_json(col("json_str"), schema).alias("data")) \
        .select("data.*")

    final_df = (
        json_df
        .withColumn("send_timestamp", to_timestamp(col("send_timestamp")))
        .withColumn("label", col("label").cast("integer"))
    )

    # Streaming Query ì‹œì‘
    query = (
        final_df.writeStream
        .outputMode("append")
        .trigger(processingTime=args.trigger_interval)
        .option("checkpointLocation", args.checkpoint_location)
        .foreachBatch(lambda batch_df, batch_id: process_batch(batch_df, batch_id, args, logger))
        .start()
    )

    logger.info("Streaming ì‹œì‘ë¨. ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    
    try:
        query.awaitTermination()
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¢…ë£Œë¨")
        query.stop()
        
        # Connection Pool ì •ë¦¬
        global _connection_pool
        if _connection_pool:
            _connection_pool.closeall()
            logger.info("Connection Pool ì¢…ë£Œ")


if __name__ == "__main__":
    main()